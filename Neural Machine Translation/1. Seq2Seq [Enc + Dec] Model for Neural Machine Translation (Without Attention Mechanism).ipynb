{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq Model for Neural Machine Translation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsengiyumva-wilberforce/AWS-DataKit-Dashboard/blob/master/Neural%20Machine%20Translation/1.%20Seq2Seq%20%5BEnc%20%2B%20Dec%5D%20Model%20for%20Neural%20Machine%20Translation%20(Without%20Attention%20Mechanism).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxGjrP6hWeB9"
      },
      "source": [
        "# A Comprehensive Guide to Neural Machine Translation using Seq2Seq Modelling using PyTorch\n",
        "\n",
        "In this post, we will be building a sequence to sequence deep learning model using PyTorch and TorchText. Here I am doing an German to English neural machine translation. But the same concept can be extended to other problems such as Named Entity Recognition (NER), Text Summarization etc,."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rr5xoWgk4Lb"
      },
      "source": [
        "# Table of Contents:\n",
        "## 1. Introduction\n",
        "## 2. Data Preparation and Pre-processing\n",
        "## 3. Long Short Term Memory (LSTM) - Under the Hood\n",
        "## 4. Encoder Model Architecture (Seq2Seq)¶\n",
        "## 5. Encoder Code Implementation (Seq2Seq)\n",
        "## 6. Decoder Model Architecture (Seq2Seq)\n",
        "## 7. Decoder Code Implementation (Seq2Seq)\n",
        "## 8. Seq2Seq (Encoder + Decoder) Interface\n",
        "## 9. Seq2Seq (Encoder + Decoder) Code Implementation\n",
        "## 10. Seq2Seq Model Training\n",
        "## 11. Seq2Seq Model Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pSOAqoHlD3y"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2fGJ_1qXRxu"
      },
      "source": [
        "Here I am doing a German to English neural machine translation. But the same concept can be extended to other problems such as Named Entity Recognition (NER), Text Summarization, etc,.\n",
        "\n",
        "So the Sequence to Sequence (seq2seq) model in this post uses an encoder-decoder architecture, which uses a type of RNN called LSTM (Long Short Term Memory), where the encoder neural network encodes the input german sequence into a single vector, also called as a Context Vector.\n",
        "This Context Vector is said to contain the abstract representation of the input german sequence.\n",
        "\n",
        "This vector is then passed into the decoder neural network, which is used to output the corresponding English translation sentence, one word at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSuJ-9X_qk1b"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Ycz13hbUbC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "54a60f76-47f1-4052-b024-df2290a025f7"
      },
      "source": [
        "!pip install torchtext==0.6.0 --quiet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from pprint import pprint\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "'''\n",
        "# Seeding for reproducible results everytime\n",
        "SEED = 777\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Seeding for reproducible results everytime\\nSEED = 777\\n\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\ntorch.cuda.manual_seed(SEED)\\ntorch.backends.cudnn.deterministic = True'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42fLcaN_kPxf"
      },
      "source": [
        "# 2. Data Preparation & Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FHjZ6RnqoNJ"
      },
      "source": [
        "Loading the SpaCy's vocabulary for our desired languages. SpaCy also supports many languages like french, german etc,.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrNraUABrDq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f8000a6-74b7-489e-aa03-dee4b6b9f53a"
      },
      "source": [
        "!python -m spacy download en --quiet\n",
        "!python -m spacy download de --quiet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Da1d8Pb-p4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "1f66356f-4cde-4865-9521-fcff9d7ee307"
      },
      "source": [
        "spacy_german = spacy.load(\"de\")\n",
        "spacy_english = spacy.load(\"en\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E941] Can't find model 'de'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"de_core_news_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models and if you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"de\")",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-3654660205.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspacy_german\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"de\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspacy_english\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'de'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"de_core_news_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models and if you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"de\")"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu2SLNiZrd0Q"
      },
      "source": [
        "Now let's create custom tokenization methods for the languages. Tokenization is a process of breaking the sentence into a list of individual tokens (words).\n",
        "\n",
        "We can make use of PyTorch's TorchText library for data pre-processing and SpaCy for vocabulary building (English and German) & tokenization of our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leVROD_6qz16"
      },
      "source": [
        "def tokenize_german(text):\n",
        "  return [token.text for token in spacy_german.tokenizer(text)]\n",
        "\n",
        "def tokenize_english(text):\n",
        "  return [token.text for token in spacy_english.tokenizer(text)]\n",
        "\n",
        "### Sample Run ###\n",
        "\n",
        "sample_text = \"I love machine learning\"\n",
        "print(tokenize_english(sample_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfbseIvPUuFz"
      },
      "source": [
        "Torch text is a powerful library for making the text data ready for variety of NLP tasks. It has all the tools to perform preprocessing on the textual data.\n",
        "\n",
        "Let's see some of the process it can do,\n",
        "\n",
        "1. Train/ Valid/ Test Split: partition your data into a specified train/ valid/ test set.\n",
        "\n",
        "2. File Loading: load the text corpus of various formats (.txt,.json,.csv).\n",
        "3. Tokenization: breaking sentences into list of words.\n",
        "4. Vocab: Generate a list of vocabulary from the text corpus.\n",
        "5. Words to Integer Mapper: Map words into integer numbers for the entire corpus and vice versa.\n",
        "6. Word Vector: Convert a word from higher dimension to lower dimension (Word Embedding).\n",
        "7. Batching: Generate batches of sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRa1BNkOU3nK"
      },
      "source": [
        "So once we get to understand what can be done in torch text, let's talk about how it can be implemented in the torch text module. Here we are going to make use of 3 classes under torch text.\n",
        "\n",
        "1. Fields :\n",
        "> This is a class under the torch text, where we specify how the preprocessing should be done on our data corpus.\n",
        "2. TabularDataset :\n",
        "> Using this class, we can actually define the Dataset of columns stored in CSV, TSV, or JSON format and also map them into integers.\n",
        "3. BucketIterator :\n",
        "> Using this class, we can perform padding our data for approximation and make batches with our data for model training.\n",
        "\n",
        "Here our source language (SRC - Input) is German and target language (TRG - Output) is English. We also add 2 extra tokens \"start of sequence\" <sos> and \"end of sequence\" <EOS> for effective model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn8CDZ1ssIju"
      },
      "source": [
        "german = Field(tokenize=tokenize_german,\n",
        "               lower=True,\n",
        "               init_token=\"<sos>\",\n",
        "               eos_token=\"<eos>\")\n",
        "\n",
        "english = Field(tokenize=tokenize_english,\n",
        "               lower=True,\n",
        "               init_token=\"<sos>\",\n",
        "               eos_token=\"<eos>\")\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = (\".de\", \".en\"),\n",
        "                                                    fields=(german, english))\n",
        "\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=3)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILXUMSRVLhb-"
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(german.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(english.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiRsZjvEME18"
      },
      "source": [
        "# dir(english.vocab)\n",
        "\n",
        "print(english.vocab.__dict__.keys())\n",
        "print(list(english.vocab.__dict__.values()))\n",
        "e = list(english.vocab.__dict__.values())\n",
        "for i in e:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhhJy36TM4SV"
      },
      "source": [
        "word_2_idx = dict(e[3])\n",
        "idx_2_word = {}\n",
        "for k,v in word_2_idx.items():\n",
        "  idx_2_word[v] = k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb-ecGHHxQCS"
      },
      "source": [
        "# Dataset sneek peek"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvt8AUrWvbA_"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
        "\n",
        "print(train_data[5].__dict__.keys())\n",
        "pprint(train_data[5].__dict__.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5CbOTA-nCMF"
      },
      "source": [
        "After setting the language pre-processing criteria, the next step is to create batches of training, testing and validation data using iterators.\n",
        "\n",
        "Creating batches is an exhaustive process, luckily we can make use of TorchText's iterator libraries.\n",
        "\n",
        "Here we are using BucketIterator for effective padding of source and target sentences. We can access the source (german) batch of data using .src attribute and it's correspondign (english) batch of data using .trg attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gmz5adIwbwF"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                                                      batch_size = BATCH_SIZE,\n",
        "                                                                      sort_within_batch=True,\n",
        "                                                                      sort_key=lambda x: len(x.src),\n",
        "                                                                      device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvYIL8X1-LAP"
      },
      "source": [
        "## Actual text data before tokenized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vWNHTlL8nSg"
      },
      "source": [
        "count = 0\n",
        "max_len_eng = []\n",
        "max_len_ger = []\n",
        "for data in train_data:\n",
        "  max_len_ger.append(len(data.src))\n",
        "  max_len_eng.append(len(data.trg))\n",
        "  if count < 10 :\n",
        "    print(\"German - \",*data.src, \" Length - \", len(data.src))\n",
        "    print(\"English - \",*data.trg, \" Length - \", len(data.trg))\n",
        "    print()\n",
        "  count += 1\n",
        "\n",
        "print(\"Maximum Length of English sentence {} and German sentence {} in the dataset\".format(max(max_len_eng),max(max_len_ger)))\n",
        "print(\"Minimum Length of English sentence {} and German sentence {} in the dataset\".format(min(max_len_eng),min(max_len_ger)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYL8BmZI0Bzh"
      },
      "source": [
        "count = 0\n",
        "for data in train_iterator:\n",
        "  if count < 1 :\n",
        "    print(\"Shapes\", data.src.shape, data.trg.shape)\n",
        "    print()\n",
        "    print(\"German - \",*data.src, \" Length - \", len(data.src))\n",
        "    print()\n",
        "    print(\"English - \",*data.trg, \" Length - \", len(data.trg))\n",
        "    temp_ger = data.src\n",
        "    temp_eng = data.trg\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr7ue9mDRNLp"
      },
      "source": [
        "temp_eng_idx = (temp_eng).cpu().detach().numpy()\n",
        "temp_ger_idx = (temp_ger).cpu().detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27RquKagotom"
      },
      "source": [
        "I just experimented with a batch size of 32 and a sample target batch is shown below. The sentences are tokenized into list of words and indexed according to the vocabulary. The \"pad\" token gets an index of 1.\n",
        "\n",
        "Each column corresponds to a sentence indexed into numbers and we have 32 such sentences in a single target batch and the number of rows corresponds to the maximum length of that sentence. Short sentences are padded with 1 to compensate.\n",
        "The table (Idx.csv) contains the numerical indices of the words, which is later fed into the word embedding and converted into dense representation for Seq2Seq processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dg028v3Ru7c"
      },
      "source": [
        "df_eng_idx = pd.DataFrame(data = temp_eng_idx, columns = [str(\"S_\")+str(x) for x in np.arange(1, 33)])\n",
        "df_eng_idx.index.name = 'Time Steps'\n",
        "df_eng_idx.index = df_eng_idx.index + 1\n",
        "# df_eng_idx.to_csv('/content/idx.csv')\n",
        "df_eng_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtkVuHkaT7ba"
      },
      "source": [
        "df_eng_word = pd.DataFrame(columns = [str(\"S_\")+str(x) for x in np.arange(1, 33)])\n",
        "df_eng_word = df_eng_idx.replace(idx_2_word)\n",
        "# df_eng_word.to_csv('/content/Words.csv')\n",
        "df_eng_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0sxi0e0lkvK"
      },
      "source": [
        "# 3. Long Short Term Memory (LSTM) - Under the Hood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL7l0Fj-lcng"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/2560/1*sQBwBtwCwqqXY0k5O0ZvMg.png\">\n",
        "\n",
        "The above picture shows the units present under a single LSTM Cell. I will add some references to learn more about LSTM in the last and why it works well for long sequences.\n",
        "\n",
        "But to simply put, Vanilla RNN, Gated Recurrent Unit (GRU) is not able to capture the long term dependencies due to its nature of design and suffers heavily by the Vanishing Gradient problem, which makes the rate of change in weights and bias values negligible, resulting in poor generalization.\n",
        "\n",
        "But LSTM has some special units called gates (Remember gate, Forget gate, Update gate), which helps to overcome the problems stated before.\n",
        "\n",
        "Inside the LSTM cell, we have a bunch of mini neural networks with sigmoid and TanH activations at the final layer and few vector adder, Concat, multiplications operations.\n",
        "\n",
        "1. Sigmoid NN → Squishes the values between 0 and 1. Say a value closer to 0 means to forget and a value closer to 1 means to remember.\n",
        "\n",
        "2. Embedding NN → Converts the input word indices into word embedding.\n",
        "\n",
        "3. TanH NN → Squishes the values between -1 and 1. Helps to regulate the vector values from either getting exploded to the maximum or shrank to the minimum.\n",
        "\n",
        "4. The hidden state and the cell state are referred to here as the context vector, which are the outputs from the LSTM cell. The input is the sentence's numerical indexes fed into the embedding NN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef7R9ZGy7Ca"
      },
      "source": [
        "# 4. Encoder Model Architecture (Seq2Seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFPrrZlAp-8o"
      },
      "source": [
        "Before moving to seq2seq model, we need to create Encoder ,Decoder and create a interface between them in the seq2seq model.\n",
        "\n",
        "Let's pass the german input sequence \"Ich liebe tief lernen\" which translates to \"I love deep learning\" in english.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMvkk_AM1orm"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*aNcybCTdPlrXsCwIo1OfTg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj94RR0ZYizi"
      },
      "source": [
        "For a lighter note, let's explain the process happening in the above image. The Encoder of the Seq2Seq model takes one input at a time. Our input German word sequence is \"ich Liebe Tief Lernen\".\n",
        "\n",
        "Also, we append the start of sequence \"SOS\" and the end of sentence \"EOS\" tokens in the starting and in the ending of the input sentence.\n",
        "\n",
        "Therefore at\n",
        "At time step-0, the \"SOS\" token is sent,\n",
        "At time step-1 the token \"ich\" is sent,\n",
        "At time step-2 the token \"Liebe\" is sent,\n",
        "At time step-3 the token \"Tief\" is sent,\n",
        "At time step-4 the token \"Lernen\" is sent,\n",
        "At time step-4 the token \"EOS\" is sent.\n",
        "\n",
        "And the first block in the Encoder architecture is the word embedding layer [shown in green block], which converts the input indexed word into a dense vector representation called word embedding (sizes - 100/200/300).\n",
        "\n",
        "Then our word embedding vector is sent to the LSTM cell, where it is combined with the hidden state (hs), and the cell state (cs) of the previous time step and the encoder block outputs a new hs and a cs which is passed to the next LSTM  cell.\n",
        "\n",
        "It is understood that the hs and cs captured some vector representation of the sentence so far.\n",
        "\n",
        "At time step-0, the hidden state and cell state are either initialized fully of zeros or random numbers.\n",
        "\n",
        "Then after we sent pass all our input german word sequence, a context vector [shown in yellow block] (hs, cs) is finally obtained, which is a dense representation of the word sequence and can be sent to the decoder's first LSTM (hs, cs) for the corresponding English translation.\n",
        "\n",
        "In the above figure, we use 2 layer LSTM  architecture, where we connect the first LSTM to the second LSTM and we then we obtain 2 context vectors stacked on top as the final output.\n",
        "\n",
        "It is a must that we design identical encoder and decoder blocks in the seq2seq model.\n",
        "\n",
        "The above visualization is applicable for a single sentence from a batch. Say we have a batch size of 5 (Experimental), then we pass 5 sentences at a time to the Encoder, which looks like the below figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wDhJhtXd1zL"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*xP8MgIfKwjStFDUo0_W3QA.png\">\n",
        "\n",
        "##  The same concept is extended to a batch size of 5 (experimental), where we consider 5 input sentences and the first token from each sentences is sent to the encoder at a time. Each sequences in the batch is maintained to have the same length using the padding token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbfs74GwmKcq"
      },
      "source": [
        "# 5. Encoder Code Implementation (Seq2Seq)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vlOtY31y40q"
      },
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.tag = True\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "  # Shape of x (26, 32) [Sequence_length, batch_size]\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "    return hidden_state, cell_state\n",
        "\n",
        "input_size_encoder = len(german.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "print(encoder_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bm8AObR4U9X"
      },
      "source": [
        "# 6. Decoder Model Architecture (Seq2Seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDSnNEG6iAO3"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*FtDDCniBMb8HXYEM6PRohQ.png\">\n",
        "\n",
        "The decoder also does a single step at a time.\n",
        "\n",
        "The Context Vector from the Encoder block is provided as the hidden state (hs) and cell state (cs) for the decoder's first LSTM block.\n",
        "\n",
        "The start of the sentence \"SOS\"  token is passed to the embedding NN, then passed to the first LSTM cell of the decoder, and finally, it is passed through a linear layer [Shown in Pink color], which provides an output English token prediction probabilities (4556 Probabilities), hidden state (hs), Cell State (cs).\n",
        "\n",
        "The output word with the highest probability is chosen, hidden state (hs), Cell State (cs) is passed as the inputs to the next LSTM cell and this process is executed until it reaches the end of sentences \"EOS\".\n",
        "\n",
        "The subsequent layers will use the hidden and cell state from the previous time steps.\n",
        "\n",
        "The above visualization is applicable for a single sentence from a batch. Say we have a batch size of 5 (Experimental), then we pass 5 sentences at a time to the Encoder, which provide 5 sets of Context Vectors, and they all are passed into the Decoder, which looks like the below figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp04ma6hyz_A"
      },
      "source": [
        "## Teach Force Ratio:\n",
        "\n",
        "In addition to other blocks, you will also see the block shown below in the Decoder of the Seq2Seq architecture.\n",
        "\n",
        "While model training, we send the inputs (German Sequence) and targets (English Sequence). After the context vector is obtained from the Encoder, we send them Vector and the target to the Decoder for translation.\n",
        "\n",
        "But during model Inference, the target is generated from the decoder based on the generalization of the training data. So the output predicted words are sent as the next input word to the decoder until a <SOS> token is obtained.\n",
        "\n",
        "So in model training itself, we can use the teach force ratio (tfr), where we can actually control the flow of input words to the decoder.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/600/1*YJpyqouvpmu4_Ej9ockl4A.png\">\n",
        "\n",
        "Teach Force Ratio methodWe can send the actual target words to the decoder part while training (Shown in Green Color).\n",
        "\n",
        "We can also send the predicted target word, as the input to the decoder (Shown in Red Color).\n",
        "\n",
        "Whether sending either of the words (actual target or predicted target) can be regulated with a probability of 50% so at any time step one of them is passed during the training.\n",
        "\n",
        "This method is like a Regularization so that the model trains efficiently during the process.\n",
        "\n",
        "The above visualization is applicable for a single sentence from a batch. Say we have a batch size of 5 (Experimental), then we pass 5 sentences at a time to the Encoder, which provide 5 sets of Context Vectors, and they all are passed into the Decoder, which looks like the below figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V1yo6_tkL_o"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/2560/1*UPyGSZSuIQ52IjyFdPpm6A.png\">\n",
        "\n",
        "The same concept is extended to a batch size of 5 (experimental), where we consider 5 input encoder's context vectors and the first token <\"sos\"> is sent to the decoder at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjW6kjByunW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIhD2-OBmVnS"
      },
      "source": [
        "# 7. Decoder Code Implementation (Seq2Seq)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnGwwU6p2Zfh"
      },
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # Shape of x (32) [batch_size]\n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "\n",
        "    # Shape of x (1, 32) [1, batch_size]\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "\n",
        "    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "    predictions = self.fc(outputs)\n",
        "\n",
        "    # Shape --> predictions (32, 4556) [batch_size , output_size]\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden_state, cell_state\n",
        "\n",
        "input_size_decoder = len(english.vocab)\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = 0.5\n",
        "output_size = len(english.vocab)\n",
        "\n",
        "decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
        "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
        "print(decoder_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sok8t_j76Ozp"
      },
      "source": [
        "# 8. Seq2Seq (Encoder + Decoder) Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZcNPOpB_DeW"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*d9kP4XoWGnIcmyhX-g4Xvw.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYYkTeNokhRu"
      },
      "source": [
        "## The final seq2seq implementation looks like the figure above.\n",
        "\n",
        "## 1. Provide both input (German) and output (English) sentences.\n",
        "## 2. Pass the input sequence to the encoder and extract context vectors.\n",
        "## 3. Pass the output sequence to the decoder, context vecotr from encoder to produce the predicted output sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhJZFJxG_HbA"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*7SVU_REkIUALmegTbFI9Fw.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0rHNbZe7ALr"
      },
      "source": [
        "for batch in train_iterator:\n",
        "  print(batch.src.shape)\n",
        "  print(batch.trg.shape)\n",
        "  break\n",
        "\n",
        "x = batch.trg[1]\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3al--sEzmfmU"
      },
      "source": [
        "# 9. Seq2Seq (Encoder + Decoder) Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuHGodQe4r9v"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.Encoder_LSTM = Encoder_LSTM\n",
        "    self.Decoder_LSTM = Decoder_LSTM\n",
        "\n",
        "  def forward(self, source, target, tfr=0.5):\n",
        "    # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n",
        "    batch_size = source.shape[1]\n",
        "\n",
        "    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(english.vocab)\n",
        "\n",
        "    # Shape --> outputs (14, 32, 5766)\n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n",
        "    hidden_state, cell_state = self.Encoder_LSTM(source)\n",
        "\n",
        "    # Shape of x (32 elements)\n",
        "    x = target[0] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      # Shape --> output (32, 5766)\n",
        "      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
        "      outputs[i] = output\n",
        "      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
        "      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "\n",
        "    # Shape --> outputs (14, 32, 5766)\n",
        "    return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOQL9vk49H2U"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpsjQCsZ_srZ"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtH0Bnq3qFmd"
      },
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    spacy_ger = spacy.load(\"de\")\n",
        "\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "def bleu(data, model, german, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, german, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
        "    print('saving')\n",
        "    print()\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, '/content/checkpoint-NMT')\n",
        "    torch.save(model.state_dict(),'/content/checkpoint-NMT-SD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6VnFyCnNlTz"
      },
      "source": [
        "# 10. Seq2Seq Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4jLBPRD9osT"
      },
      "source": [
        "|epoch_loss = 0.0\n",
        "num_epochs = 100\n",
        "best_loss = 999999\n",
        "best_epoch = -1\n",
        "sentence1 = \"ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster\"\n",
        "ts1  = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
        "  model.eval()\n",
        "  translated_sentence1 = translate_sentence(model, sentence1, german, english, device, max_length=50)\n",
        "  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n",
        "  ts1.append(translated_sentence1)\n",
        "\n",
        "  model.train(True)\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    input = batch.src.to(device)\n",
        "    target = batch.trg.to(device)\n",
        "\n",
        "    # Pass the input and target for model's forward method\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    # Clear the accumulating gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss value for every epoch\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient value is it exceeds > 1\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Update the weights values using the gradients we calculated using bp\n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "    epoch_loss += loss.item()\n",
        "    writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    best_epoch = epoch\n",
        "    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss)\n",
        "    if ((epoch - best_epoch) >= 10):\n",
        "      print(\"no improvement in 10 epochs, break\")\n",
        "      break\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n",
        "\n",
        "print(epoch_loss / len(train_iterator))\n",
        "\n",
        "score = bleu(test_data[1:100], model, german, english, device)\n",
        "print(f\"Bleu score {score*100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqylUksMJtVo"
      },
      "source": [
        "#%load_ext tensorboard\n",
        "%tensorboard --logdir runs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yyYsPQ2ml7Y"
      },
      "source": [
        "# 11. Seq2Seq Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8seg8haidFT"
      },
      "source": [
        "progress  = []\n",
        "import nltk\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "for i, sen in enumerate(ts1):\n",
        "  progress.append(TreebankWordDetokenizer().detokenize(sen))\n",
        "print(progress)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH2U-LbhH1dw"
      },
      "source": [
        "progress_df = pd.DataFrame(data = progress, columns=['Predicted Sentence'])\n",
        "progress_df.index.name = \"Epochs\"\n",
        "progress_df.to_csv('/content/predicted_sentence.csv')\n",
        "progress_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAUieohaNpvd"
      },
      "source": [
        "# Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxcYB6cRJKIZ"
      },
      "source": [
        "model.eval()\n",
        "test_sentences  = [\"Zwei Männer gehen die Straße entlang\", \"Kinder spielen im Park.\", \"Diese Stadt verdient eine bessere Klasse von Verbrechern. Der Spaßvogel\"]\n",
        "actual_sentences  = [\"Two men are walking down the street\", \"Children play in the park\", \"This city deserves a better class of criminals. The joker\"]\n",
        "pred_sentences = []\n",
        "\n",
        "for idx, i in enumerate(test_sentences):\n",
        "  model.eval()\n",
        "  translated_sentence = translate_sentence(model, i, german, english, device, max_length=50)\n",
        "  progress.append(TreebankWordDetokenizer().detokenize(translated_sentence))\n",
        "  print(\"German : {}\".format(i))\n",
        "  print(\"Actual Sentence in English : {}\".format(actual_sentences[idx]))\n",
        "  print(\"Predicted Sentence in English : {}\".format(progress[-1]))\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}